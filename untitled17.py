# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uRdo1cb4F2B52X9KS5xTIBo59lAimCUd
"""

"""
nas_forecasting.py
Complete NAS-based Time Series Forecasting pipeline (PyTorch).
- Synthetic data generation or load your own multivariate dataset (shape: T x features).
- Baseline LSTM and Transformer (with Positional Encoding).
- Evaluation metrics: MSE, RMSE, MAE, MASE.
- NAS: random search + mutation-based evolutionary refinement.
- Attention extraction & visualization for best Transformer.
Notes:
- This does not guarantee 100% score on your evaluation platform (no one can).
- It implements the deliverables required and produces a ready-to-submit model + report artifacts.
"""

import os
import math
import random
import copy
import argparse
from typing import Dict, Any, List, Tuple

import numpy as np
import matplotlib.pyplot as plt

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from tqdm import trange, tqdm

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

# -------------------------
# 1) Data generation / loading
# -------------------------
def generate_synthetic_series(T=5000, n_features=5, noise_std=0.1):
    t = np.linspace(0, 50, T)
    data = []
    for i in range(n_features):
        freq = np.random.uniform(0.5, 2.0)
        phase = np.random.uniform(0, np.pi)
        amp = np.random.uniform(0.8, 1.2)
        slow = 0.3 * np.sin(0.2 * t + np.random.uniform(0, 2*np.pi))
        series = amp * np.sin(freq * t + phase) + slow + np.random.normal(0, noise_std, size=T)
        data.append(series)
    data = np.array(data).T  # (T, features)
    # normalize per feature (zero mean, unit std)
    mean = data.mean(axis=0, keepdims=True)
    std = data.std(axis=0, keepdims=True) + 1e-6
    data = (data - mean) / std
    return data.astype(np.float32)


class SeriesDataset(Dataset):
    def __init__(self, data: np.ndarray, input_len: int = 64, output_len: int = 1):
        # data: (T, features)
        self.data = data
        self.input_len = input_len
        self.output_len = output_len
        self.N = len(data) - input_len - output_len + 1

    def __len__(self):
        return max(0, self.N)

    def __getitem__(self, idx):
        x = self.data[idx: idx + self.input_len]           # (input_len, features)
        y = self.data[idx + self.input_len: idx + self.input_len + self.output_len]  # (output_len, features)
        return torch.from_numpy(x), torch.from_numpy(y)


# -------------------------
# 2) Positional Encoding
# -------------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 10000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, d_model)
        return x + self.pe[:, :x.size(1), :]


# -------------------------
# 3) Models: LSTM & Transformer
# -------------------------
class LSTMForecast(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        # x: (batch, seq, features)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # predict next step for all features
        return out.unsqueeze(1)  # (batch, 1, features)


class TransformerForecast(nn.Module):
    def __init__(self, input_dim: int, d_model: int = 64, n_heads: int = 4, num_layers: int = 2, dropout: float = 0.1, feedforward_dim: int = 256):
        super().__init__()
        self.embed = nn.Linear(input_dim, d_model)
        self.pos = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward=feedforward_dim,
                                                   dropout=dropout, activation='relu', batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.out = nn.Linear(d_model, input_dim)

        # for attention extraction
        self._attn_weights = []

    def forward(self, x):
        # x: (batch, seq, features)
        x = self.embed(x)               # (batch, seq, d_model)
        x = self.pos(x)
        # clear attn buffer
        self._attn_weights = []

        # register hooks to capture attention weights from each encoder layer's self_attn
        # Note: we will capture weights inside the encoder by temporarily patching forward of self_attn
        for layer in self.encoder.layers:
            # monkey patch a small wrapper around layer.self_attn.forward to capture weights
            orig_forward = layer.self_attn.forward

            def make_forward(orig):
                def wrapped(q, k, v, attn_mask=None, key_padding_mask=None, need_weights=True, attn_bias=None):
                    # call original; PyTorch's MultiheadAttention returns (attn_output, attn_output_weights)
                    attn_out, attn_weights = orig(q, k, v, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=True)
                    # attn_weights shape: (batch_heads, seq_len, seq_len) OR (batch, num_heads, tgt_len, src_len) depending on version
                    try:
                        self._attn_weights.append(attn_weights.detach().cpu())
                    except:
                        pass
                    return attn_out, attn_weights
                return wrapped

            layer.self_attn.forward = make_forward(orig_forward)

        encoded = self.encoder(x)  # (batch, seq, d_model)
        out = self.out(encoded[:, -1, :])  # (batch, features)
        return out.unsqueeze(1)  # (batch, 1, features)


# -------------------------
# 4) Metrics
# -------------------------
def mse_metric(pred: torch.Tensor, target: torch.Tensor) -> float:
    return F.mse_loss(pred, target).item()

def rmse_metric(pred: torch.Tensor, target: torch.Tensor) -> float:
    return torch.sqrt(F.mse_loss(pred, target)).item()

def mae_metric(pred: torch.Tensor, target: torch.Tensor) -> float:
    return F.l1_loss(pred, target).item()

def mase_metric(pred: torch.Tensor, target: torch.Tensor, seasonality: int = 1) -> float:
    """
    MASE implementation for multivariate: compute per-feature then average.
    naive forecast uses seasonal lag `seasonality`. For seasonality=1 it's a naive one-step.
    pred, target: tensors (batch, out_len=1, features) or (N, features)
    """
    # flatten batch dimension
    p = pred.detach().cpu().squeeze(1).numpy()
    t = target.detach().cpu().squeeze(1).numpy()
    # p, t: (N, features)
    N, Fv = p.shape
    mases = []
    for f in range(Fv):
        actual = t[:, f]
        forecast = p[:, f]
        # numerator
        num = np.mean(np.abs(actual - forecast))
        # denominator: mean absolute naive in-sample difference over training series
        # For denominator we need a reference series; we will compute using shifts within test actuals (approx)
        if len(actual) <= seasonality:
            denom = np.mean(np.abs(actual - np.roll(actual, 1))) + 1e-9
        else:
            denom = np.mean(np.abs(actual[seasonality:] - actual[:-seasonality])) + 1e-9
        mases.append(num / denom)
    return float(np.mean(mases))


# -------------------------
# 5) Trainer / Evaluator
# -------------------------
def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, device: torch.device):
    model.train()
    running_loss = 0.0
    count = 0
    for x, y in loader:
        x = x.to(device); y = y.to(device)
        optimizer.zero_grad()
        pred = model(x)  # (batch, 1, features)
        loss = F.mse_loss(pred, y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * x.size(0)
        count += x.size(0)
    return running_loss / count if count > 0 else 0.0

def evaluate_model(model: nn.Module, loader: DataLoader, device: torch.device):
    model.eval()
    preds = []
    trues = []
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device); y = y.to(device)
            p = model(x)
            preds.append(p.cpu())
            trues.append(y.cpu())
    if len(preds) == 0:
        return {}
    preds = torch.cat(preds, dim=0)  # (N, 1, features)
    trues = torch.cat(trues, dim=0)
    return {
        "mse": mse_metric(preds, trues),
        "rmse": rmse_metric(preds, trues),
        "mae": mae_metric(preds, trues),
        "mase": mase_metric(preds, trues, seasonality=1),
        "preds": preds,
        "trues": trues
    }


# -------------------------
# 6) NAS search: Random + mutation-based evolutionary
# -------------------------
def sample_random_config(input_dim: int) -> Dict[str, Any]:
    # Search space
    model_type = random.choice(["lstm", "transformer"])
    cfg = {"model_type": model_type}
    if model_type == "lstm":
        cfg.update({
            "hidden_dim": random.choice([32, 64, 128]),
            "num_layers": random.choice([1, 2, 3]),
            "dropout": random.choice([0.0, 0.1, 0.2])
        })
    else:
        cfg.update({
            "d_model": random.choice([32, 64, 96]),
            "n_heads": random.choice([2, 4]),
            "num_layers": random.choice([1, 2, 3]),
            "dropout": random.choice([0.0, 0.1]),
            "ff_dim": random.choice([128, 256])
        })
    cfg["input_dim"] = input_dim
    return cfg

def mutate_config(cfg: Dict[str, Any]) -> Dict[str, Any]:
    new = copy.deepcopy(cfg)
    if cfg["model_type"] == "lstm":
        if random.random() < 0.5:
            new["hidden_dim"] = random.choice([32, 64, 128])
        else:
            new["num_layers"] = random.choice([1, 2, 3])
    else:
        k = random.choice(["d_model", "n_heads", "num_layers", "ff_dim"])
        if k == "d_model":
            new["d_model"] = random.choice([32, 64, 96])
        elif k == "n_heads":
            new["n_heads"] = random.choice([2, 4])
        elif k == "num_layers":
            new["num_layers"] = random.choice([1, 2, 3])
        else:
            new["ff_dim"] = random.choice([128, 256])
    return new


def build_model_from_cfg(cfg: Dict[str, Any]) -> nn.Module:
    if cfg["model_type"] == "lstm":
        return LSTMForecast(input_dim=cfg["input_dim"],
                            hidden_dim=cfg["hidden_dim"],
                            num_layers=cfg["num_layers"],
                            dropout=cfg.get("dropout", 0.1))
    else:
        return TransformerForecast(input_dim=cfg["input_dim"],
                                   d_model=cfg["d_model"],
                                   n_heads=cfg["n_heads"],
                                   num_layers=cfg["num_layers"],
                                   dropout=cfg.get("dropout", 0.1),
                                   feedforward_dim=cfg.get("ff_dim", 256))

# -------------------------
# 7) Full pipeline: search, retrain best, save results
# -------------------------
def run_pipeline(
    data: np.ndarray,
    input_len: int = 64,
    output_len: int = 1,
    train_frac: float = 0.8,
    batch_size: int = 64,
    search_iters: int = 10,
    evo_pop: int = 6,
    device: torch.device = None,
    quick_epochs: int = 3,
    final_epochs: int = 20,
    save_dir: str = "./nas_output"
):
    os.makedirs(save_dir, exist_ok=True)
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    # splits (simple contiguous split)
    T = len(data)
    train_end = int(T * train_frac)
    val_end = int(T * (train_frac + (1 - train_frac) / 2))

    train_data = data[:train_end]
    val_data = data[train_end:val_end]
    test_data = data[val_end:]

    train_ds = SeriesDataset(train_data, input_len=input_len, output_len=output_len)
    val_ds = SeriesDataset(val_data, input_len=input_len, output_len=output_len)
    test_ds = SeriesDataset(test_data, input_len=input_len, output_len=output_len)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)

    input_dim = data.shape[1]

    # Random search seed population
    population = [sample_random_config(input_dim) for _ in range(evo_pop)]
    history = []

    # evaluate config function (quick training for a few epochs)
    def eval_cfg(cfg):
        torch.manual_seed(SEED)
        model = build_model_from_cfg(cfg).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        # quick training
        for ep in range(quick_epochs):
            train_one_epoch(model, train_loader, optimizer, device)
        # validate
        val_metrics = evaluate_model(model, val_loader, device)
        # use RMSE as selection metric
        return val_metrics.get("rmse", float("inf")), val_metrics, model

    # initial population evaluation
    print("Starting NAS: random search + mutation-based evolution")
    for i in range(search_iters):
        # sample some new candidates
        candidates = []
        # add some pure random
        for _ in range(max(1, evo_pop // 2)):
            candidates.append(sample_random_config(input_dim))
        # add mutated ones from population
        for p in population:
            candidates.append(mutate_config(p))
        # evaluate candidates
        cand_results = []
        for c in candidates:
            rmse, metrics, model = eval_cfg(c)
            cand_results.append((rmse, c, metrics))
            history.append((c, metrics))
            print(f"Iter {i} candidate rmse {rmse:.4f} cfg {c}")
        # select top-k to form next population (by rmse)
        cand_results.sort(key=lambda x: x[0])
        population = [c for _, c, _ in cand_results[:evo_pop]]
        print(f"Iteration {i} best RMSE: {cand_results[0][0]:.4f}")

    # find best config across history
    best = min(history, key=lambda x: x[1]["rmse"])
    best_cfg = best[0]
    best_metric = best[1]
    print("Best config found:", best_cfg)
    print("Best config val metrics:", {k: best_metric[k] for k in ["mse", "rmse", "mae", "mase"] if k in best_metric})

    # Retrain best model on train+val then evaluate on test
    combined_data = np.concatenate([train_data, val_data], axis=0)
    combined_ds = SeriesDataset(combined_data, input_len=input_len, output_len=output_len)
    combined_loader = DataLoader(combined_ds, batch_size=batch_size, shuffle=True)

    best_model = build_model_from_cfg(best_cfg).to(device)
    optimizer = torch.optim.Adam(best_model.parameters(), lr=1e-3)

    print("Retraining best model for final evaluation...")
    for ep in range(final_epochs):
        tr_loss = train_one_epoch(best_model, combined_loader, optimizer, device)
        if (ep+1) % 5 == 0 or ep == final_epochs - 1:
            print(f"Epoch {ep+1}/{final_epochs} train_loss={tr_loss:.6f}")

    # Save model
    model_path = os.path.join(save_dir, "best_model.pth")
    torch.save({"model_state": best_model.state_dict(), "cfg": best_cfg}, model_path)
    print("Saved best model to", model_path)

    # Evaluate on test
    test_metrics = evaluate_model(best_model, test_loader, device)
    print("Test metrics:", {k: test_metrics[k] for k in ["mse", "rmse", "mae", "mase"]})
    # Save metrics and config
    np.savez(os.path.join(save_dir, "results.npz"), cfg=best_cfg, mse=test_metrics["mse"], rmse=test_metrics["rmse"],
             mae=test_metrics["mae"], mase=test_metrics["mase"])

    # If Transformer, extract attention maps on a sample
    if best_cfg["model_type"] == "transformer":
        best_model.eval()
        # get a small batch from test
        for x, y in test_loader:
            x = x.to(device)
            _ = best_model(x[:1])  # forward to populate _attn_weights
            # try to fetch weights
            attn_ws = getattr(best_model, "_attn_weights", [])
            # attn_ws: list of weight tensors; depends on torch version shape
            if len(attn_ws) > 0:
                # show first layer's attention (if shape matches expected)
                w = attn_ws[0]
                # try to reshape to (num_heads, seq, seq)
                try:
                    arr = w[0].numpy()
                except:
                    arr = w.cpu().numpy()
                plt.figure(figsize=(6,5))
                plt.title("Sample Attention (layer 0)")
                plt.imshow(arr, aspect='auto')
                plt.colorbar()
                plt.tight_layout()
                plt.savefig(os.path.join(save_dir, "attention_layer0.png"))
                print("Saved attention visualization to", os.path.join(save_dir, "attention_layer0.png"))
            break

    # produce final short textual summary file for submission (report-ready)
    summary = {
        "best_cfg": best_cfg,
        "test_mse": float(test_metrics["mse"]),
        "test_rmse": float(test_metrics["rmse"]),
        "test_mae": float(test_metrics["mae"]),
        "test_mase": float(test_metrics["mase"])
    }
    with open(os.path.join(save_dir, "summary.txt"), "w") as f:
        f.write("NAS Forecasting - summary\n")
        f.write(f"Best config: {best_cfg}\n")
        f.write(f"Test metrics: MSE={summary['test_mse']:.6f}, RMSE={summary['test_rmse']:.6f}, MAE={summary['test_mae']:.6f}, MASE={summary['test_mase']:.6f}\n")
        f.write("\nNotes:\n- Model retrained on train+val for final evaluation.\n- Attention visualization saved if Transformer.\n")
    print("Wrote summary to", os.path.join(save_dir, "summary.txt"))

    return summary


# -------------------------
# 8) Main & run
# -------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_len", type=int, default=64)
    parser.add_argument("--output_len", type=int, default=1)
    parser.add_argument("--search_iters", type=int, default=8)
    parser.add_argument("--final_epochs", type=int, default=20)
    parser.add_argument("--quick_epochs", type=int, default=2)
    parser.add_argument("--save_dir", type=str, default="./nas_output")
    args = parser.parse_args()

    # generate synthetic data (you can replace this with loading a real dataset)
    data = generate_synthetic_series(T=5000, n_features=5)
    print("Generated synthetic data shape:", data.shape)

    summary = run_pipeline(data,
                           input_len=args.input_len,
                           output_len=args.output_len,
                           search_iters=args.search_iters,
                           quick_epochs=args.quick_epochs,
                           final_epochs=args.final_epochs,
                           save_dir=args.save_dir)
    print("Done. Summary:", summary)